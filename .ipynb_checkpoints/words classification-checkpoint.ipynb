{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26855872-ec74-4de1-a018-dd4a7b504fc0",
   "metadata": {},
   "source": [
    "<center><b>Model to categorize words based on their fisrt character - one hot encoding</b></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d17cda92-9670-4870-b8fa-7fe9a5dde711",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dependencies\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import string\n",
    "\n",
    "# Use device agnostic code\n",
    "device= \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "15cf6de5-067b-42bd-9af8-beea9bb7261a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the hyperparameters for data creation\n",
    "STARTING_CHAR = 'c'\n",
    "ALPHABET = string.ascii_lowercase\n",
    "NUM_CLASSES = 2 # <- begins with STARTING_CHAR or not (two classes)\n",
    "NUM_FEATURES = 3 # <- lets have 3 characters in each word\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "\n",
    "# Function to get one-hot encoding of a character\n",
    "def char_to_vec(character):\n",
    "    character = character.lower()\n",
    "    char_vec = [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]\n",
    "    alph_index = ALPHABET.index(character)\n",
    "    char_vec[alph_index] = 1\n",
    "    return char_vec\n",
    "\n",
    "# Create the input data\n",
    "raw_inputs = [\"Cat\", \"put\", \"Rat\", \"cut\", \"Car\", \"Tuc\", \"mat\", \"cot\", \"key\", \"Cup\", \"bit\", \"lab\",\n",
    "                  \"cow\", \"Ten\", \"cap\", \"one\", \"run\", \"Can\", \"Cab\", \"cub\"]\n",
    "\n",
    "# Convert the inputs to their corrsponding one-hot encoding format\n",
    "total_words = len(raw_inputs)\n",
    "\n",
    "char_vectors = [] # <- create a list to store the vectors\n",
    "word_labels = [] # <- a label is in the format [0,1] for words starting with STARTING_CHAR and [1,0] for those that start with other characters\n",
    "\n",
    "for w in range(total_words):\n",
    "    word_char_vecs = []\n",
    "    for c in range(NUM_FEATURES):        \n",
    "        word_char_vecs.append(char_to_vec(raw_inputs[w][c]))\n",
    "    \n",
    "    char_vectors.append(word_char_vecs)\n",
    "    if raw_inputs[w][0].lower() == STARTING_CHAR:\n",
    "        word_labels.append([0,1])\n",
    "    else:\n",
    "        word_labels.append([1,0])\n",
    "\n",
    "# Convert the lists to numpy arrays\n",
    "char_vectors_array = np.array(char_vectors)\n",
    "word_labels_array = np.array(word_labels)\n",
    "\n",
    "# Turn them into tensors\n",
    "X = torch.from_numpy(char_vectors_array).type(torch.float)\n",
    "y = torch.from_numpy(word_labels_array).type(torch.FloatTensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3e42327b-82ec-4848-9cc1-a223e46988b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16, 4, 16, 4)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=RANDOM_SEED)\n",
    "\n",
    "len(X_train), len(X_test), len(y_train), len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "683d5088-0aec-4720-8af5-6173f5304f4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WordCFModel(\n",
       "  (layer_stack): Sequential(\n",
       "    (0): Linear(in_features=26, out_features=50, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=50, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build the model\n",
    "class WordCFModel(nn.Module):\n",
    "    def __init__(self, input_features, output_features, hidden_units=8):\n",
    "        super().__init__()\n",
    "        self.layer_stack = nn.Sequential(\n",
    "                nn.Linear(in_features=input_features, out_features=hidden_units),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(in_features=hidden_units, out_features=hidden_units),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(in_features=hidden_units, out_features=output_features)            \n",
    "            )\n",
    "    def forward(self, x):\n",
    "        return self.layer_stack(x)\n",
    "\n",
    "# Create an instance of the model\n",
    "model_cf = WordCFModel(input_features=26, output_features=NUM_CLASSES, hidden_units=50).to(device)\n",
    "\n",
    "model_cf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "64f7eab2-9a67-4ecf-8f3a-6f2e12543b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the loss function\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "# Create optimizer function\n",
    "optimizer = torch.optim.SGD(model_cf.parameters(), lr=0.1) # Stochastic Gradient Descent optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "511cabc7-66a4-491a-94db-7a865f37cec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create accuracy function\n",
    "def accuracy_fn(y_true, y_pred):\n",
    "    correct = torch.eq(torch.softmax(y_true, dim=1).argmax(dim=1), y_pred).sum().item() # torch.eq() calculates where two tensors are equal\n",
    "    acc = (correct / len(y_pred)) * 100\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5a45fdf6-31d5-4db2-a69d-b1c51de445f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.0717, 0.1061],\n",
       "         [0.0711, 0.1034],\n",
       "         [0.0667, 0.0980]],\n",
       "\n",
       "        [[0.0717, 0.1061],\n",
       "         [0.0711, 0.1034],\n",
       "         [0.0571, 0.0805]],\n",
       "\n",
       "        [[0.0718, 0.1070],\n",
       "         [0.0571, 0.0805],\n",
       "         [0.0658, 0.0935]],\n",
       "\n",
       "        [[0.0513, 0.0784],\n",
       "         [0.0327, 0.0568],\n",
       "         [0.0667, 0.0980]]], grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test if the model is working by performing a forward pass\n",
    "model_cf(X_test.to(device))[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08bb587f-64f3-4a3f-ba3c-7bb1c4d1c49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the training and testing loop\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "\n",
    "epochs = 100\n",
    "\n",
    "# Put data to target device\n",
    "X_train, y_train = X_train.to(device), y_train.to(device)\n",
    "X_test, y_test = X_test.to(device), y_test.to(device)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    ### Training\n",
    "    model_cf.train()\n",
    "\n",
    "    # 1. Forward pass\n",
    "    train_logits = model_cf(X_train) # model outputs raw logits\n",
    "    train_preds = torch.soft_max(train_logits, dim=1).argmax(dim=1) # go from logits -> prediction probabilities -> prediction labels\n",
    "\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
