{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26855872-ec74-4de1-a018-dd4a7b504fc0",
   "metadata": {},
   "source": [
    "<center><b>Model to categorize words based on their fisrt character - one hot encoding</b></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d17cda92-9670-4870-b8fa-7fe9a5dde711",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dependencies\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import string\n",
    "\n",
    "# Use device agnostic code\n",
    "device= \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "15cf6de5-067b-42bd-9af8-beea9bb7261a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the hyperparameters for data creation\n",
    "STARTING_CHAR = 'c'\n",
    "ALPHABET = string.ascii_lowercase\n",
    "NUM_CLASSES = 2 # <- begins with STARTING_CHAR or not (two classes)\n",
    "NUM_FEATURES = 26 # <- size of a character vector\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "\n",
    "# Function to get one-hot encoding of a character\n",
    "def char_to_vec(character):\n",
    "    character = character.lower()\n",
    "    char_vec = [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]\n",
    "    alph_index = ALPHABET.index(character)\n",
    "    char_vec[alph_index] = 1\n",
    "    return char_vec\n",
    "\n",
    "# Create the input data\n",
    "raw_inputs = [\"Cat\", \"put\", \"Rat\", \"cut\", \"Car\", \"Tuc\", \"mat\", \"cot\", \"key\", \"Cup\", \"bit\", \"lab\",\n",
    "                  \"cow\", \"Ten\", \"cap\", \"one\", \"run\", \"Can\", \"Cab\", \"cub\"]\n",
    "\n",
    "# Convert the inputs to their corrsponding one-hot encoding format\n",
    "total_words = len(raw_inputs)\n",
    "\n",
    "char_vectors = [] # <- create a list to store the vectors\n",
    "word_labels = [] # <- a label is in the format [0,1] for words starting with STARTING_CHAR and [1,0] for those that start with other characters\n",
    "\n",
    "for w in range(total_words):       \n",
    "    char_vectors.append(char_to_vec(raw_inputs[w][0])) # We only need the first character vector to classify the word   \n",
    "    \n",
    "    if raw_inputs[w][0].lower() == STARTING_CHAR:\n",
    "        word_labels.append([0,1])\n",
    "    else:\n",
    "        word_labels.append([1,0])\n",
    "\n",
    "# Convert the lists to numpy arrays\n",
    "char_vectors_array = np.array(char_vectors)\n",
    "word_labels_array = np.array(word_labels)\n",
    "\n",
    "# Turn them into tensors\n",
    "X = torch.from_numpy(char_vectors_array).type(torch.float)\n",
    "y = torch.from_numpy(word_labels_array).type(torch.LongTensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3e42327b-82ec-4848-9cc1-a223e46988b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16, 4, 16, 4)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=RANDOM_SEED)\n",
    "\n",
    "len(X_train), len(X_test), len(y_train), len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "683d5088-0aec-4720-8af5-6173f5304f4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WordCFModel(\n",
       "  (layer_stack): Sequential(\n",
       "    (0): Linear(in_features=26, out_features=50, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=50, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build the model\n",
    "class WordCFModel(nn.Module):\n",
    "    def __init__(self, input_features, output_features, hidden_units=50):\n",
    "        super().__init__()\n",
    "        self.layer_stack = nn.Sequential(\n",
    "                nn.Linear(in_features=input_features, out_features=hidden_units),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(in_features=hidden_units, out_features=hidden_units),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(in_features=hidden_units, out_features=output_features)            \n",
    "            )\n",
    "    def forward(self, x):\n",
    "        return self.layer_stack(x)\n",
    "\n",
    "# Create an instance of the model\n",
    "model_cf = WordCFModel(input_features=NUM_FEATURES, output_features=NUM_CLASSES).to(device)\n",
    "\n",
    "model_cf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "64f7eab2-9a67-4ecf-8f3a-6f2e12543b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the loss function\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "# Create optimizer function\n",
    "optimizer = torch.optim.SGD(model_cf.parameters(), lr=0.1) # Stochastic Gradient Descent optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "511cabc7-66a4-491a-94db-7a865f37cec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create accuracy function\n",
    "def accuracy_fn(y_true, y_pred):\n",
    "    correct = torch.eq(torch.softmax(y_true, dim=1).argmax(dim=1), y_pred).sum().item() # torch.eq() calculates where two tensors are equal\n",
    "    acc = (correct / len(y_pred)) * 100\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5a45fdf6-31d5-4db2-a69d-b1c51de445f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1291, 0.0110],\n",
       "        [0.1291, 0.0110],\n",
       "        [0.1342, 0.0230],\n",
       "        [0.1337, 0.0217]], grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test if the model is working by performing a forward pass\n",
    "model_cf(X_test.to(device))[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "797fdbfe-1727-4f97-aa82-16fe99b7e1c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2]), 2)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_cf(X_train.to(device))[0].shape, NUM_CLASSES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "08bb587f-64f3-4a3f-ba3c-7bb1c4d1c49a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.0016468979883939028 | Train accuracy: 100.0% | Test loss: 0.006987008266150951 | Test accuracy: 100.0%\n",
      "Train loss: 0.0015970219392329454 | Train accuracy: 100.0% | Test loss: 0.006834934465587139 | Test accuracy: 100.0%\n",
      "Train loss: 0.0015497896820306778 | Train accuracy: 100.0% | Test loss: 0.006688508205115795 | Test accuracy: 100.0%\n",
      "Train loss: 0.001505016814917326 | Train accuracy: 100.0% | Test loss: 0.006550604477524757 | Test accuracy: 100.0%\n",
      "Train loss: 0.0014625245239585638 | Train accuracy: 100.0% | Test loss: 0.006415564566850662 | Test accuracy: 100.0%\n",
      "Train loss: 0.0014220901066437364 | Train accuracy: 100.0% | Test loss: 0.006287767086178064 | Test accuracy: 100.0%\n",
      "Train loss: 0.0013835870195180178 | Train accuracy: 100.0% | Test loss: 0.00616812240332365 | Test accuracy: 100.0%\n",
      "Train loss: 0.001346956705674529 | Train accuracy: 100.0% | Test loss: 0.006053464487195015 | Test accuracy: 100.0%\n",
      "Train loss: 0.0013120354851707816 | Train accuracy: 100.0% | Test loss: 0.005940117873251438 | Test accuracy: 100.0%\n",
      "Train loss: 0.0012787191662937403 | Train accuracy: 100.0% | Test loss: 0.005832730792462826 | Test accuracy: 100.0%\n"
     ]
    }
   ],
   "source": [
    "# Create the training and testing loop\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "\n",
    "epochs = 100\n",
    "\n",
    "# Put data to target device\n",
    "X_train, y_train = X_train.to(device), y_train.to(device)\n",
    "X_test, y_test = X_test.to(device), y_test.to(device)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    ### Training\n",
    "    model_cf.train()\n",
    "\n",
    "    # 1. Forward pass\n",
    "    train_logits = model_cf(X_train).squeeze() # model outputs raw logits\n",
    "    train_preds = torch.softmax(train_logits, dim=1).argmax(dim=1) # go from logits -> prediction probabilities -> prediction labels\n",
    "\n",
    "    # 2. Calculate loss and accuracy\n",
    "    loss = loss_fn(train_logits, y_train.type(torch.FloatTensor))\n",
    "    acc = accuracy_fn(y_true=y_train.type(torch.FloatTensor),\n",
    "                 y_pred=train_preds.type(torch.FloatTensor)\n",
    "                )\n",
    "\n",
    "    # 3. Optimizer zero grad\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # 4. Loss backwards\n",
    "    loss.backward()\n",
    "\n",
    "    # 5. Optimizer step\n",
    "    optimizer.step()\n",
    "\n",
    "    ### Testing\n",
    "    model_cf.eval()\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        # 1. Forward pass\n",
    "        test_logits = model_cf(X_test).squeeze()\n",
    "        test_preds = torch.softmax(test_logits, dim=1).argmax(dim=1)\n",
    "        \n",
    "        # 2. Calculate loss and accuracy\n",
    "        test_loss = loss_fn(test_logits, y_test.type(torch.FloatTensor))\n",
    "        test_acc = accuracy_fn(y_true=y_test.type(torch.FloatTensor),\n",
    "                          y_pred=test_preds.type(torch.FloatTensor)\n",
    "                         )\n",
    "\n",
    "        # Print what's happening every 10 epochs\n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Train loss: {loss} | Train accuracy: {acc}% | Test loss: {test_loss} | Test accuracy: {test_acc}%\")\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7d1f8e2f-1cd9-4545-a3f6-695f18b7007a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model to: models/word_classifier_model_0.pth\n"
     ]
    }
   ],
   "source": [
    "# Save the model\n",
    "from pathlib import Path\n",
    "\n",
    "# 1.Models directory\n",
    "MODEL_PATH = Path(\"models\")\n",
    "\n",
    "# 2. Create model save path\n",
    "MODEL_NAME = \"word_classifier_model_0.pth\"\n",
    "MODEL_SAVE_PATH = MODEL_PATH / MODEL_NAME\n",
    "\n",
    "# 3. Save the model state dict\n",
    "print(f\"Saving model to: {MODEL_SAVE_PATH}\")\n",
    "torch.save(obj=model_cf.state_dict(), # only saving the state_dict() only saves the models learned parameters\n",
    "           f=MODEL_SAVE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "641444fb-9639-4e25-92cf-f6aa76bd5473",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WordCFModel(\n",
       "  (layer_stack): Sequential(\n",
       "    (0): Linear(in_features=26, out_features=50, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=50, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load and use model\n",
    "# Create a new instance of our model class\n",
    "loaded_model_cf = WordCFModel(input_features=NUM_FEATURES,\n",
    "                              output_features=NUM_CLASSES\n",
    "                             ).to(device)\n",
    "\n",
    "# Load saved model state_dict\n",
    "loaded_model_cf.load_state_dict(torch.load(f=MODEL_SAVE_PATH), strict=False)\n",
    "\n",
    "loaded_model_cf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cf0ef084-3751-41ab-bdb6-92b57db63968",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 100.0% \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0.]]),\n",
       " tensor([1, 1, 0, 0]))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use it for inference\n",
    "loaded_model_cf.eval()\n",
    "\n",
    "with torch.inference_mode():\n",
    "    # 1. Forward pass\n",
    "    y_logits = loaded_model_cf(X_test)\n",
    "    y_preds = torch.softmax(y_logits, dim=1).argmax(dim=1)\n",
    "\n",
    "    # 2. Calculate accuracy\n",
    "    acc = accuracy_fn(y_true=y_test.type(torch.FloatTensor),\n",
    "                             y_pred=y_preds.type(torch.FloatTensor)\n",
    "                         )\n",
    "    \n",
    "\n",
    "# Print the result\n",
    "print(f\"Accuracy: {acc}% \")\n",
    "X_test, y_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "109e4b1f-97b5-47c3-81e7-2fa02b776c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on new data\n",
    "\n",
    "# Create the input data\n",
    "raw_inputs = [\"Cop\", \"nut\", \"cun\", \"xed\", \"cak\", \"cit\"]\n",
    "\n",
    "# Convert the inputs to their corrsponding one-hot encoding format\n",
    "total_words = len(raw_inputs)\n",
    "\n",
    "char_vectors = [] # <- create a list to store the vectors\n",
    "\n",
    "for w in range(total_words):\n",
    "    char_vectors.append(char_to_vec(raw_inputs[w][0]))       \n",
    "  \n",
    "# Convert the lists to numpy arrays\n",
    "char_vectors_array = np.array(char_vectors)\n",
    "\n",
    "# Turn them into tensors\n",
    "X = torch.from_numpy(char_vectors_array).type(torch.float)\n",
    "\n",
    "X_inf = X.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "260c0f4d-3a3e-44c3-b2f4-db5e2cbb6f30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 0, 1, 0, 1, 1])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use it for inference\n",
    "loaded_model_cf.eval()\n",
    "\n",
    "with torch.inference_mode():\n",
    "    # 1. Forward pass\n",
    "    y_logits = loaded_model_cf(X_inf)\n",
    "    y_preds = torch.softmax(y_logits, dim=1).argmax(dim=1)\n",
    "\n",
    "y_preds    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b83f159e-0742-4d57-a4cf-773e70fd7f4b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
